\documentclass{article}
\usepackage{tikz}
\usepackage{mathtools}

\newcommand\then{\rightarrow}
\newcommand\liff{\leftrightarrow}
\newcommand\lxor{\oplus}
\author{Sandra Kohl, Jan Hendrik Kirchner, Max Bernhard Ilsen}

\begin{document}
\section{Exercise \textit{(Probabilities (2p))}}
\begin{enumerate}
    \item
        \begin{align*}
            P(red|b_1) &= \frac{h(red,b_1)}{h(b1)} &= \frac{5}{5+3+2} &= \frac{5}{10} &= \frac{1}{2}\\
            P(green|b_1) &= \frac{h(green,b_1)}{h(b1)} &= \frac{3}{5+3+2} &&= \frac{3}{10}\\
            P(yellow|b_1) &= \frac{h(yellow,b_1)}{h(b1)} &= \frac{2}{5+3+2} &= \frac{2}{10} &= \frac{1}{5}\\
        \end{align*}
    \item $p(b_1) = 0.2, p(b_2) = 0.3, p(b_3) = 0.5$\\
        \\
        \begin{align*}
            P(red) &= P(red|b_1)P(b_1) + P(red|b_2)P(b_2)+ P(red|b_3)P(b_3)\\
                   &= 0.2\frac{5}{5+3+2} + 0.3\frac{1}{1+2+3} + 0.5\frac{4}{4+2+5}\\
                   &= \frac{73}{220} \\&= 0.3319\\
            P(yellow) &= P(yellow|b_1)P(b_1) + P(yellow|b_2)P(b_2)+
            P(yellow|b_3)P(b_3)\\
            &= 0.2\frac{3}{5+3+2} + 0.3\frac{2}{1+2+3} + 0.5\frac{2}{4+2+5}\\
            &= \frac{69}{275} \\&= 0.251\\
            P(yellow) &= P(yellow|b_1)P(b_1) + P(yellow|b_2)P(b_2)+
            P(green)\\ &= P(green|b_1)P(b_1) + P(green|b_2)P(b_2)+
            P(green|b_3)P(b_3)\\
            &= 0.2\frac{2}{5+3+2} + 0.3\frac{3}{1+2+3} + 0.5\frac{5}{4+2+5}\\
            &= \frac{459}{1100}\\ &= 0.417\\
        \end{align*}
\end{enumerate}

\section{Exercise \textit{(Bayes Classifier (8p))}}
\begin{enumerate}
    \item Bayes' rule for illness given som symptom s:
            $P(i|s) =  \frac{P(s|i)P(i)}{P(s)}$\\
        \begin{itemize}
            \item $P(s|i):$\\
                $P(n|i) = \frac{2}{3},
                P(c|i) = \frac{2}{3},
                P(r|i) = \frac{2}{3},
                P(f|i) = \frac{1}{3}$\\
                $P(\lnot n|i) = \frac{1}{3},
                P(\lnot c|i) = \frac{1}{3},
                P(\lnot r|i) = \frac{1}{3},
                P(\lnot f|i) = \frac{2}{3}$
            \item $P(i) = \frac{3}{6} = 0.5$\\
            \item $P(s):$\\
                $P(n) = \frac{3}{6} = 0.5,
                P(c) = \frac{3}{6} = 0.5,
                P(r) = \frac{3}{6} = 0.5,
                P(f) = \frac{1}{6}$\\
                $P(\lnot n) = \frac{3}{6} = 0.5,
                P(\lnot c) = \frac{3}{6} = 0.5,
                P(\lnot r) = \frac{3}{6} = 0.5,
                P(\lnot f) = \frac{5}{6}$
        \end{itemize}
    \item
        % did you know that you can copy latex formulas into wolframalpha?
        % just be carful with exponents
        \begin{align*}
            d1: P(i|n,c,r,\lnot f) &= \frac{P(n|i)P(c|i)P(r|i)P(\lnot
        f|i)P(i)}{P(n)P(c)P(r)P(\lnot f)} &=
            \frac{\frac{2}{3}^40.5}{0.5^3\frac{5}{6}} &= 0.94\\
            d2: P(i|n,c,\lnot r,\lnot f) &=\frac{P(n|i)P(c|i)P(\lnot r|i)P(\lnot
        f|i)P(i)}{P(n)P(c)P(\lnot r)P(\lnot f)} &=
            \frac{\frac{2}{3}^4\frac{1}{3}0.5}{0.5^3\frac{5}{6}} &= 0.316\\
            d3: P(i|\lnot n,\lnot c,r,f) &=\frac{P(\lnot n|i)P(\lnot c|i)P(r|i)P(
        f|i)P(i)}{P(\lnot n)P(\lnot c)P(r)P(f)} &=
            \frac{\frac{1}{3}^3\frac{2}{3}^20.5}{0.5^3\frac{1}{3}} &= 0.198\\
            d4: P(i|n,\lnot c,\lnot r,\lnot f) &=\frac{P(n|i)P(\lnot c|i)P(\lnot r|i)P(\lnot
        f|i)P(i)}{P(n)P(\lnot c)P(\lnot r)P(\lnot f)} &=
            \frac{\frac{1}{3}^2\frac{2}{3}^20.5}{0.5^3\frac{5}{6}} &= 0.237\\
            d5: P(i|\lnot n,\lnot c,\lnot r,\lnot f) &=\frac{P(\lnot n|i)P(\lnot c|i)P(\lnot r|i)P(\lnot
        f|i)P(i)}{P(\lnot n)P(\lnot c)P(\lnot r)P(\lnot f)} &=
            \frac{\frac{1}{3}^3\frac{2}{3}^20.5}{0.5^3\frac{5}{6}} &= 0.079\\
            d6: P(i|\lnot n,c,r,\lnot f) &=\frac{P(\lnot n|i)P(c|i)P(r|i)P(\lnot
        f|i)P(i)}{P(\lnot n)P(c)P(r)P(\lnot f)} &=
            \frac{\frac{1}{3}\frac{2}{3}^30.5}{0.5^3\frac{5}{6}} &= 0.474\\
        \end{align*}
    \item
        % \begin{align*}
        %     P(i|c,f) &=\frac{P(c|i)P(f|i)P(i)}{P(c)P(f)} &=
        %     \frac{\frac{2}{3}\frac{1}{3}0.5}{0.5\frac{1}{6}} &= 1.333\\
        %     P(i|n,f) &=\frac{P(n|i)P(f|i)P(i)}{P(n)P(f)} &=
        %     \frac{\frac{2}{3}\frac{1}{3}0.5}{0.5\frac{1}{6}} &= 1.333\\
        %     P(i|n,r) &=\frac{P(n|i)P(r|i)P(i)}{P(n)P(r)} &=
        %     \frac{\frac{2}{3}^20.5}{0.5^2} &= 0.889\\
        % \end{align*}
        % probabilities higher than 1? Where's my error, apparently I'm blind.
\end{enumerate}

\section{Exercise \textit{(Reinforcement Learning (10p))}}
\begin{enumerate}
    \item $V(s_t) = 0*0.9+0*0.9^2+0*0.9^3+0*0.9^4+0*0.9^5+100*0.9^6 = 53.1441 $
    \item Three episodes of Q-learning:\\
        % $Q(s,a) = r(s,a) + \gamma V^*(\delta(s,a))$\\
        % $a^*(s) =  argmax_a Q(s,a)$
        $q(s,a) =  r + \gamma max_a’ q(s’,a’)$\\
        States are described by their coordinates in $[1,3] \times [1,3]$.\\
        Normally, the initial state is chosen at random. Today, our totally
        legitimate nine-sided dice always lets us take (1,1).\\
        Furthermore, we choose a probabilistic approach of choosing the next
        action. We use a dice from the same company as the previously used
        nine-sided one and always end up with the same path from $(1,1)$ to
        $(3,3)$.
        \begin{enumerate}
            \item
             $q((1,1),up) = 0 + 0.9*0 = 0$\\
             $q((2,1),up) = 0 + 0.9*0 = 0$\\
             $q((3,1),right) = 0 + 0.9*0 = 0$\\
             $q((3,2),right) = 100 + 0.9*0 = 100$\\
         \item
             $q((1,1),up) = 0 + 0.9*0 = 0$\\
             $q((2,1),up) = 0 + 0.9*0 = 0$\\
             $q((3,1),right) = 0 + 0.9*100 = 90$\\
             $q((3,2),right) = 100 + 0.9*0 = 100$\\
         \item
             $q((1,1),up) = 0 + 0.9*0 = 0$\\
             $q((2,1),up) = 0 + 0.9*90 = 81$\\
             $q((3,1),right) = 0 + 0.9*100 = 90$\\
             $q((3,2),right) = 100 + 0.9*0 = 100$\\
        \end{enumerate}
        % These exercises just do not feel like they actually serve a purpose.
\end{enumerate}

\section{Exercise \textit{(Classification (6p))}}
\begin{enumerate}
    \item
        \begin{enumerate}
            \item If we want to use SVM to solve multi-class classification
                problem such as MNIST, shortly explain how can we solve this
                problem?\\
                \\
                We could use a SVM for each possible pair of classes. Then,
                during each iteration, we assign a class label to each example.
                The class such an example is assigned to most often is taken as
                its predicted output class.
        \end{enumerate}
\end{enumerate}

\section{Exercise \textit{(LDA (6p))}}
\begin{enumerate}
    \item
        Discuss the purpose of LDA in comparison with PCA.\\
        \\
        Both LDA and PCA search for linear combinations of variables that
        explain the data. They are therefore unsuited for nonlinear distributions
        in the case of PCA or distributions that are not linearly seperable in
        the case of LDA.
        While LDA is specifically used to assign class labels to examples, PCA
        does not take into account different classes and only searches for the
        directions of the largest variance of the data.
\end{enumerate}

\end{document}





