\documentclass{article}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{amsfonts}



\newcommand\then{\rightarrow}
\newcommand\liff{\leftrightarrow}
\newcommand\lxor{\oplus}
\author{Sandra Kohl, Jan Hendrik Kirchner, Max Bernhard Ilsen}

\begin{document}
\section{Exercise \textit{(Multi-Layer Perceptron (8p))}}
\section{Exercise \textit{(RBF (8p))}}
\begin{enumerate}
    \item Discuss RBF network and MLP in different aspects e.g. input and output dimension, extrapolation, lesion tolerance and advantages of each network.
	\begin{itemize}
		\item \textbf{Input-/ Output-dimension} \\
			Both methods can be interpreted as mappings from $\mathbb{R}^n \to \mathbb{R}^m$. However in a RBF network we generally don't include a bias term on the input to the hidden layer. A bias term would not make sense in that situation since the hidden layer is responsive to a particular range of values and shifting the data into that range with the help of a bias term would defeat the purpose. Therefore the dimensionality of a MLP is generally bigger by 1 dimension than the input to a RBF. \\
			While all features from the input can influence all hidden units of a MLP, A RBF is \textit{local}, i.e. its hidden units selectively respond to a particular range of values.
		\item \textbf{Extrapolation} \\
		\item \textbf{Lesion tolerance} \\
		\item \textbf{Advantages} \\

	\end{itemize}
    \item Explain the generalization and avoiding overfitting.
        
    \item To prevent overly large weights which cause the high sensitivity of inputs, we apply the
        quadratic regularization term in the error function. Use gradient descent to minimize this
        error function.
        
\end{enumerate}
\end{document}
